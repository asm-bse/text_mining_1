{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Hotel URLs with Selenium\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to scrape hotel listing URLs from Booking.com using **Selenium**. We use a headless browser automation technique to navigate through search results and extract hotel links. The extracted URLs are saved to text files for further processing.\n",
    "\n",
    "While Booking.com URLs can be generated directly with query parameters, we **chose not to** use this approach. Instead, we simulate a real user’s interaction with the webpage.\n",
    "\n",
    "If we wanted to avoid using Selenium and directly generate a URL for Booking.com, we could use the following format:\n",
    "\n",
    "```\n",
    "https://www.booking.com/searchresults.html?ss=Barcelona&checkin=2025-06-03&checkout=2025-06-09&group_adults=2&group_children=0&no_rooms=1&order=price\n",
    "```\n",
    "\n",
    "Where\n",
    "- ss=Barcelona → City to search (Barcelona)\n",
    "- checkin=2025-06-03 → Check-in date (June 3, 2025)\n",
    "- checkout=2025-06-09 → Check-out date (June 9, 2025)\n",
    "- group_adults=2 → Number of adults (2 people)\n",
    "- group_children=0 → Number of children (0 children)\n",
    "- no_rooms=1 → Number of rooms (1 room)\n",
    "- order=price → Sorting order (e.g., by price)\n",
    "\n",
    "This URL can be easily adapted for any other city, date range, number of guests, or sorting preference by replacing the corresponding values.\n",
    "\n",
    "## Scraping Strategy\n",
    "\n",
    "1. **Load the main page**: Navigate to [Booking.com](https://www.booking.com/index.es.html).\n",
    "2. **Enter search parameters**:\n",
    "   - Set the destination city.\n",
    "   - Select check-in and check-out dates via the calendar.\n",
    "3. **Scroll and load more results**:\n",
    "   - Initially, three scrolls load approximately **100 hotels**.\n",
    "   - Each subsequent scroll adds **~25 more hotels**.\n",
    "   - We keep scrolling and clicking the \"Load more\" button up to a predefined limit of 999 hotels (`number_of_cycles = 37`).\n",
    "4. **Extract hotel URLs**:\n",
    "   - Scrape the URLs of hotel listings by identifying the appropriate **CSS class**.\n",
    "   - Store the results in a text file.\n",
    "\n",
    "## Code Components\n",
    "\n",
    "- **`scroll_to_bottom(driver)`** – Scrolls the page to the bottom to load more results.\n",
    "- **`scroll_three_times(driver)`** – Performs an initial set of three scrolls.\n",
    "- **`go_to_calendar_page(driver, current_date, target_date)`** – Navigates through the calendar to select check-in and check-out dates.\n",
    "- **`scrape_hotels(city, start_date, end_date, number_of_cycles)`** – The main function that automates the scraping process.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "For each city and date range, a text file is created in `./scraped_hotel_urls/`, containing the list of hotel URLs.\n",
    "\n",
    "Example:\n",
    "```\n",
    "./scraped_hotel_urls/Barcelona_2025-06-03_to_2025-06-09_hotel_urls.txt\n",
    "```\n",
    "\n",
    "## Execution Plan\n",
    "\n",
    "We run the scraping function for two cities (**Barcelona** and **Madrid**) across two date ranges:\n",
    "- **June 3–9, 2025**\n",
    "- **June 10–16, 2025**\n",
    "\n",
    "This results in **4 separate scraping runs**.\n",
    "\n",
    "## Performance\n",
    "\n",
    "Each 37 cycles run takes about 7 minutes so we scrape 4000 hotels in less than 30 minutes\n",
    "We can impove runtime and sustainability replacing time.sleep with WebDriverWait().until, but we didnt cover it during classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scraping for Barcelona (2025-06-03 to 2025-06-09)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Barcelona (2025-06-03 to 2025-06-09): 100%|██████████| 37/37 [04:54<00:00,  7.95s/cycle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "999 hotels found for Barcelona (2025-06-03 to 2025-06-09)\n",
      "Results saved to ./scraped_hotel_urls/Barcelona_2025-06-03_to_2025-06-09_hotel_urls.txt\n",
      "====================================================================================================\n",
      "\n",
      "Starting scraping for Madrid (2025-06-03 to 2025-06-09)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Madrid (2025-06-03 to 2025-06-09): 100%|██████████| 37/37 [04:49<00:00,  7.82s/cycle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 hotels found for Madrid (2025-06-03 to 2025-06-09)\n",
      "Results saved to ./scraped_hotel_urls/Madrid_2025-06-03_to_2025-06-09_hotel_urls.txt\n",
      "====================================================================================================\n",
      "\n",
      "Starting scraping for Barcelona (2025-06-10 to 2025-06-16)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Barcelona (2025-06-10 to 2025-06-16): 100%|██████████| 37/37 [04:58<00:00,  8.07s/cycle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "992 hotels found for Barcelona (2025-06-10 to 2025-06-16)\n",
      "Results saved to ./scraped_hotel_urls/Barcelona_2025-06-10_to_2025-06-16_hotel_urls.txt\n",
      "====================================================================================================\n",
      "\n",
      "Starting scraping for Madrid (2025-06-10 to 2025-06-16)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Madrid (2025-06-10 to 2025-06-16): 100%|██████████| 37/37 [04:49<00:00,  7.83s/cycle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "999 hotels found for Madrid (2025-06-10 to 2025-06-16)\n",
      "Results saved to ./scraped_hotel_urls/Madrid_2025-06-10_to_2025-06-16_hotel_urls.txt\n",
      "====================================================================================================\n",
      "\n",
      "Scraping complete for all cities and dates.\n"
     ]
    }
   ],
   "source": [
    "#!pip install selenium tqdm\n",
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Configuration\n",
    "base_url = 'https://www.booking.com/index.es.html'\n",
    "output_dir = './scraped_hotel_urls/'  # Directory to save the text files\n",
    "hotels_webpage_class = 'a78ca197d0'\n",
    "button_xpath = '//button[contains(@class, \"bf0537ecb5\")]'\n",
    "next_month_button_xpath = '//button[contains(@class, \"f073249358\")]'\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = Chrome()\n",
    "\n",
    "def scroll_to_bottom(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "def scroll_three_times(driver):\n",
    "    for _ in range(3):\n",
    "        scroll_to_bottom(driver)\n",
    "\n",
    "def go_to_calendar_page(driver, current_date, target_date):\n",
    "    \"\"\"\n",
    "    Navigates to the appropriate calendar pages to select the required dates.\n",
    "    \"\"\"\n",
    "    current_date_obj = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "    target_date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    month_diff = (target_date_obj.year - current_date_obj.year) * 12 + (target_date_obj.month - current_date_obj.month)\n",
    "\n",
    "    for _ in range(month_diff):\n",
    "        next_month_button = driver.find_element(By.XPATH, next_month_button_xpath)\n",
    "        next_month_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "def scrape_hotels(city, start_date, end_date, number_of_cycles=5):\n",
    "    \"\"\"\n",
    "    Scrape hotel URLs for a given city and date range.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting scraping for {city} ({start_date} to {end_date})\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Input city\n",
    "    search_input = driver.find_element(By.ID, ':rh:')\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(city)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Open calendar and select dates\n",
    "    calendar_css = 'button.ebbedaf8ac:nth-child(2) > span:nth-child(1)'\n",
    "    scroll_to_bottom(driver) # Scroll to make the calendar visible on small screens\n",
    "    driver.find_element('css selector', calendar_css).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Select start and end dates\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    go_to_calendar_page(driver, current_date, start_date)\n",
    "    driver.find_element(By.XPATH, f'//span[@data-date=\"{start_date}\"]').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    go_to_calendar_page(driver, start_date, end_date)\n",
    "    driver.find_element(By.XPATH, f'//span[@data-date=\"{end_date}\"]').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Submit search\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"indexsearch\"]/div[2]/div/form/div/div[4]/button').click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll and load more results\n",
    "    with tqdm(total=number_of_cycles, desc=f\"Scraping {city} ({start_date} to {end_date})\", unit=\"cycle\") as pbar:\n",
    "        cycle_count = 0\n",
    "        while cycle_count < number_of_cycles:\n",
    "            scroll_three_times(driver) # actually we need to scroll 3 times to load all the hotels only fist time, later we need to scroll only once\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, button_xpath))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "                load_more_button.click()\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nReached end of results or error encountered: {e}\")\n",
    "                break\n",
    "            cycle_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Collect hotel URLs\n",
    "    hotels = driver.find_elements(By.CLASS_NAME, hotels_webpage_class)\n",
    "    hotel_urls = [hotel.get_attribute(\"href\") for hotel in hotels]\n",
    "\n",
    "    # Save results to a file\n",
    "    filename = f\"{output_dir}{city}_{start_date}_to_{end_date}_hotel_urls.txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for url in hotel_urls:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "    print(f\"\\n{len(hotel_urls)} hotels found for {city} ({start_date} to {end_date})\")\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    return len(hotel_urls)\n",
    "\n",
    "# Scrape hotel URLs for Barcelona and Madrid\n",
    "try:\n",
    "    scrape_hotels(\"Barcelona\", \"2025-06-03\", \"2025-06-09\", number_of_cycles=37)  # fist cycle is about 100 hotels, the 25 hotels per cycle, up to max of 36 cycles\n",
    "    scrape_hotels(\"Madrid\", \"2025-06-03\", \"2025-06-09\", number_of_cycles=37)\n",
    "    scrape_hotels(\"Barcelona\", \"2025-06-10\", \"2025-06-16\", number_of_cycles=37)\n",
    "    scrape_hotels(\"Madrid\", \"2025-06-10\", \"2025-06-16\", number_of_cycles=37)\n",
    "    print(\"\\nScraping complete for all cities and dates.\")\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Hotel Details with BeautifulSoup\n",
    "\n",
    "## Overview\n",
    "\n",
    "This script extracts detailed information about hotels from **Booking.com** using **requests** and **BeautifulSoup**. It processes previously scraped hotel URLs stored in text files and retrieves information such as the hotel's name, description, room type, rating, and price. The extracted data is then saved to a CSV file.\n",
    "\n",
    "## Why Use Requests and BeautifulSoup?\n",
    "\n",
    "Instead of using Selenium for this step, we opted for **requests** and **BeautifulSoup** to:\n",
    "- **Improve efficiency** – Requests are generally faster than Selenium-based browser automation.\n",
    "- **Avoid unnecessary browser rendering** – Since we only need structured data, using a lightweight parser is more efficient.\n",
    "\n",
    "## Scraping Strategy\n",
    "\n",
    "1. **Read URL files**:\n",
    "   - The script looks for text files in the `./scraped_hotel_urls/` directory.\n",
    "   - Each file contains a list of hotel URLs from a specific city and date range.\n",
    "\n",
    "2. **Extract metadata from filenames**:\n",
    "   - City, check-in date, and check-out date are extracted from the filename format:\n",
    "     ```\n",
    "     City_YYYY-MM-DD_to_YYYY-MM-DD_hotel_urls.txt\n",
    "     ```\n",
    "\n",
    "3. **Loop through URLs**:\n",
    "   - Send a **GET request** with a **random user agent** to avoid detection.\n",
    "   - Parse the response using **BeautifulSoup**.\n",
    "\n",
    "4. **Extract hotel details**:\n",
    "   - **Hotel Name**: Extracted from the page header.\n",
    "   - **Description**: Retrieved from the property’s summary section.\n",
    "   - **Room Type**: Identified from the room listing.\n",
    "   - **Rating**: Extracted from the user review section.\n",
    "   - **Price**: Scraped from the pricing display.\n",
    "\n",
    "5. **Store results in a DataFrame**:\n",
    "   - Each hotel’s data is stored in a Pandas **DataFrame**.\n",
    "   - The data is then **saved as a CSV file** (`scraped_hotel_data.csv`).\n",
    "\n",
    "## Code Components\n",
    "\n",
    "- **`USER_AGENTS`** – A list of different User-Agent strings to reduce blocking risks.\n",
    "- **`requests.get(url, headers=headers)`** – Sends an HTTP request with a random User-Agent.\n",
    "- **`BeautifulSoup(response.content, \"html.parser\")`** – Parses the HTML response.\n",
    "- **Regular Expressions (`re.search()`)** – Used to extract numeric rating values.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "After running the script, a CSV file `scraped_hotel_data.csv` is generated, containing:\n",
    "- **URL**\n",
    "- **Hotel Name**\n",
    "- **Description**\n",
    "- **Room Type**\n",
    "- **City**\n",
    "- **Check-in Date**\n",
    "- **Check-out Date**\n",
    "- **Rating**\n",
    "- **Price**\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "- **Anti-Scraping Protection** – Booking.com may block repeated requests, so using **rotating user agents** and **delays** can help.\n",
    "- **HTML Structure Changes** – If the website layout changes, selectors might need to be updated.\n",
    "- **Missing Data** – Some elements may not be present on all hotel pages, leading to `\"N/A\"` values.\n",
    "\n",
    "## Performance\n",
    "\n",
    "Each 999 hotels run takes about 25 minutes so we scrape 4000 hotels in less than 2 hours\n",
    "We can impove runtime and sustainability using async requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/4 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Processing file 1 of 4: Barcelona_2025-06-03_to_2025-06-09_hotel_urls.txt (999 URLs)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 1/4 Progress: 100%|██████████| 999/999 [32:18<00:00,  1.94s/URL]\n",
      "Processing files:  25%|██▌       | 1/4 [32:18<1:36:54, 1938.29s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Processing file 2 of 4: Barcelona_2025-06-10_to_2025-06-16_hotel_urls.txt (992 URLs)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 2/4 Progress: 100%|██████████| 992/992 [31:51<00:00,  1.93s/URL]\n",
      "Processing files:  50%|█████     | 2/4 [1:04:10<1:04:05, 1922.70s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Processing file 3 of 4: Madrid_2025-06-03_to_2025-06-09_hotel_urls.txt (1000 URLs)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 3/4 Progress: 100%|██████████| 1000/1000 [30:14<00:00,  1.81s/URL]\n",
      "Processing files:  75%|███████▌  | 3/4 [1:34:24<31:13, 1873.41s/file]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Processing file 4 of 4: Madrid_2025-06-10_to_2025-06-16_hotel_urls.txt (999 URLs)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 4/4 Progress: 100%|██████████| 999/999 [30:43<00:00,  1.85s/URL]\n",
      "Processing files: 100%|██████████| 4/4 [2:05:08<00:00, 1877.01s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Data saved to scraped_hotel_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install bs4 requests pandas tqdm\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import random\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/108.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Gecko/20100101 Firefox/108.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/114.0.0.0\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Directory containing URL files\n",
    "directory = './scraped_hotel_urls/'\n",
    "\n",
    "# DataFrame to store the results\n",
    "df = pd.DataFrame(columns=[\"url\", \"hotel_name\", \"description\", \"room_type\", \"city\", \"start_date\", \"end_date\", \"rating\", \"price\"])\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "total_files = len(files)  # Total number of files to process\n",
    "\n",
    "# Loop through the filenames in the directory with a progress bar\n",
    "for file_index, filename in enumerate(tqdm(files, desc=\"Processing files\", unit=\"file\")):\n",
    "    # Create the full path to the file\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    # Extract city, start_date, and end_date from the filename\n",
    "    city = filename.split('_')[0]\n",
    "    match = re.search(r'_(\\d{4}-\\d{2}-\\d{2})_to_(\\d{4}-\\d{2}-\\d{2})_', filename)\n",
    "    start_date, end_date = match.groups() if match else (\"N/A\", \"N/A\")\n",
    "    \n",
    "    # Read the URLs from the file\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    total_urls = len(lines)\n",
    "    \n",
    "    # Progress bar for URLs in the file\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nProcessing file {file_index + 1} of {total_files}: {filename} ({total_urls} URLs)\")\n",
    "    print(\"=\" * 100)\n",
    "    for line in tqdm(lines, desc=f\"File {file_index + 1}/{total_files} Progress\", unit=\"URL\"):\n",
    "        url = line.strip()\n",
    "        \n",
    "        # Send GET request to the URL\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(USER_AGENTS)\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # If request is successful, parse the HTML\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Extract hotel name\n",
    "            hotel_name_element = soup.find(\"h2\", class_=\"pp-header__title\")\n",
    "            hotel_name = hotel_name_element.get_text(strip=True) if hotel_name_element else \"N/A\"\n",
    "            \n",
    "            # Extract property description\n",
    "            description_element = soup.find(\"p\", {\"data-testid\": \"property-description\"})\n",
    "            hotel_description = description_element.get_text(strip=True) if description_element else \"N/A\"\n",
    "            \n",
    "            # Extract room type\n",
    "            room_type_element = soup.find(\"span\", class_=\"hprt-roomtype-icon-link\")\n",
    "            hotel_short_description = room_type_element.get_text(strip=True) if room_type_element else \"N/A\"\n",
    "            \n",
    "            # Extract rating\n",
    "            rating_element = soup.find(\"div\", class_=\"ac4a7896c7\")\n",
    "            if rating_element:\n",
    "                rating_match = re.search(r'\\d+(\\.\\d+)?', rating_element.text)\n",
    "                hotel_rating = rating_match.group() if rating_match else \"N/A\"\n",
    "            else:\n",
    "                hotel_rating = \"N/A\"\n",
    "            \n",
    "            # Extract price\n",
    "            price_element = soup.find(\"div\", class_=\"bui-price-display__value\")\n",
    "            hotel_price = price_element.get_text(strip=True) if price_element else \"N/A\"\n",
    "            \n",
    "            # Append the extracted data to the DataFrame\n",
    "            df = pd.concat([df, pd.DataFrame([{\n",
    "                \"url\": url,\n",
    "                \"hotel_name\": hotel_name,\n",
    "                \"description\": hotel_description,\n",
    "                \"room_type\": hotel_short_description,\n",
    "                \"city\": city,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"rating\": hotel_rating,\n",
    "                \"price\": hotel_price\n",
    "            }])], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {url}, Status Code: {response.status_code}\")\n",
    "\n",
    "# Replace placeholder rating with NaN and save to CSV\n",
    "df['rating'] = df['rating'].replace(999, 'NaN')  # Adjust as needed for placeholder values\n",
    "output_file = 'scraped_hotel_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nProcessing complete. Data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsdm_main_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
